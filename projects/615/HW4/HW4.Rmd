Homework 4 (due 8 Oct in class)
========================================================

### Find the stationary distribution for an autoregressive process of order 1. ###

An AR(1) process can be represented as: $y_t = \phi y_{t-1} + \epsilon_t$ where $\epsilon_t$ is white noise (ie, $\epsilon_t \sim N(0, \sigma^2)$). 

Note this can also be representing in the state-space model framework where the observation equation is: $y_t = 1*y_t + 0*y_{t-1} + 0$ or $y_t = F_t \theta_t + v_t$ where $F_t = 1$, $\theta_t = y_t$, and $v_t = 0$. The state equation would then be: $y_t = \phi y_{t-1} + \epsilon_t$ or $\theta_t = G_t \theta_{t-1} + w_t$ where $G_t = \phi$ and $w_t \sim N(0, \sigma^2)$

Treating this as a _dynamic linear model_, we could specify a normal prior distribution for the state at time $t=0$: $y_0 \sim N(m_0, C_0)$.

It then follows that $\theta_t|\theta_{t-1} \sim N(G_t\theta_{t-1}, W_t) \implies y_t|y_{t-1} \sim N(\phi y_{t-1}, \sigma^2)$ 

### Perform a Bayesian analysis of this [data set](http://jarad.me/stat615/data/dlm-data.csv), temperature measurements 25cm below the surface on an experimental plot in Wyoming. Here is some code to get you started: ###

Since the variation in the seasonal component is consistent over time (see figure below), should provide an adequate fit to this data.

```{r setup, echo=FALSE, include=FALSE}
library(knitr)
render_markdown()
```

```{r decompose, echo=FALSE}
#setwd("~/Desktop/github/local/cpsievert.github.com/projects/615/HW4")
library(dlm)
d = read.csv("dlm-data.csv") 
d$date = as.Date(as.character(d$date), format="%m/%d/%y")
y = d$EHSR.25cm.T 
y_tmp = y 
y_tmp[1180] = mean(y[c(1179,1181)]) #impute missing value
n = nrow(d) 
de = decompose(ts(y_tmp, start=d$date[1], end=d$date[n], freq=24)) 
plot(de) 
```

To keep things simple, consider the random walk plus noise model:

$$
y_t = \mu_t + v_t \hspace{2cm} v_t \sim N(0, V)
\\ \mu_t = \mu_{t-1} + w_t \hspace{2cm} w_t \sim N(0, W)
$$

Note that this model is constant and the only parameters are the observation and evolution variances ($V$ and $W$, respectively). These (unknown) parameters are usually estimated via Maximum Likelihood and/or Bayesian estimation. Before we head in this direction, first consider the trajectory of the random walk plus model with a low signal-to-noise ratio $r = W/V = 1/100$

```{r one, fig.width=10}
dlmTemp <- dlmModPoly(order = 1, dV = 100, dW = 1)
tempFilt <- dlmFilter(y_tmp, dlmTemp)
tempSmooth <- dlmSmooth(y_tmp, dlmTemp)
plot(y_tmp, type="l")
lines(tempFilt$m[-1], col=2)
lines(tempSmooth$s[-1], col=3)
legend("bottomright", c("Filtered Trajectory", "Smoothed Trajectory"), col=2:3, lty=1)
```

In this case, the choice for these values of $V$ and $W$ were arbitrary. To obtain a better estimate at the true value of these paramters, we can use Maximum Likelihood:

```{r two, fig.width=10, cache=TRUE}
buildFun <- function(x) {
  dlmModPoly(order = 1, dV = exp(x[1]), dW = exp(x[2]))
}
fit <- dlmMLE(y_tmp, parm=rep(0, 3), build=buildFun)
stopifnot(fit$convergence == 0)
dlmTemp2 <- buildFun(fit$par)
unlist(dlmTemp2[c("V", "W")])
tempFilt2 <- dlmFilter(y, dlmTemp2)
tempSmooth2 <- dlmSmooth(y, dlmTemp2)
plot(y_tmp, type="l")
lines(tempFilt2$m[-1], col=2)
lines(tempSmooth2$s[-1], col=3)
legend("bottomright", c("Filtered Trajectory", "Smoothed Trajectory"), col=2:3, lty=1)
```

Note that the high signal-to-noise ratio estimated via Maximum Likelihood produces a trajectory that essentially mimics the actual trajectory. Due to this overfitting, any prediction would be highly inaccurate since predictions would be heavily influenced by the noise in previous observations. If we were to treat these parameter estimates as known and perform a Bayesian analysis, one could obtain samples from the posterior via the Forward Filtering Backward Sampling (FFBS) algorithm. However, this is not a great way to proceed since (in addition to these values not really being known) backwards sampling from this filtered object would produce samples with very little variation. Instead, we proceed by treating $V$ and $W$ as unknown. 

Assuming that $W$ is a diagonal matrix and both unknown variances have independent inverse Gamma prior distributions, we can sample from the posterior using the function `dlm::dlmGibbsDIG`.

```{r gibbs, cache=TRUE}
mcmc <- 1000
burn <- 500
outGibbs <- dlmGibbsDIG(y_tmp, mod = dlmModPoly(1), shape.y = 1e-3, rate.y = 1e-3,
shape.theta = 1e-3, rate.theta = 1e-3, n.sample = mcmc + burn)
```

```{r stuff}
m <- mcmcMean(with(outGibbs, cbind(V = dV[-(1:burn)], W = dW[-(1:burn), ])))
m[1,] #point estimates for unknown variances
m[2,] #Monte Carlo standard errors
```

Again, we obtain a low (point) estimate for the observation variance $V$ (and a very high signal-to-noise ratio). In the figure below, the upper and lower bound to the 95% credible interval for the unknown states are drawn for the actual data.

```{r plot, fig.width=10}
thetas <- outGibbs$theta[,,-(1 : burn)]
mns <- rowMeans(thetas)
l <- apply(thetas, 1, quantile, .025)
u <- apply(thetas, 1, quantile, .975)
plot(y_tmp, type="l")
lines(l, col=3, lty=2)
lines(u, col=3, lty=2)
```

Clearly, a simple random walk plus noise model is not going to work since the observation and evolution variance are so small. Next, we consider adding a fourier representation of the periodic component. This particular representation contains two harmonics.

```{r explore}
buildFun <- function(x) {
  dlmModPoly(order = 1, dV = exp(x[1]), dW = exp(x[2])) + dlmModTrig(s=24, q=2, dV=exp(x[1]), dW=exp(x[2]))
}
fit <- dlmMLE(log(y_tmp), parm = rep(1, 2), build = buildFun, lower = 1e-8, upper = 10)
stopifnot(fit$convergence == 0)
t <- buildFun(fit$par)
```

Note that the estimates of $V$ and $W$ are still incredibly small, but this model systematically restricts the states from mimicing the actual trajectory (as seen below). It is interesting to see the large spike in the filtered trajectory at the start of the time series, but it makes total sense considering the filtered estimates are based on _previous_ data and there large spike from the 1st time point to the 2nd time point.

```{r plot_harm, fig.width=10}
tempFilt2 <- dlmFilter(y_tmp, t)
tempSmooth2 <- dlmSmooth(y_tmp, t)
plot(y_tmp, type="l")
lines(tempFilt2$m[-1], col=2)
lines(tempSmooth2$s[-1], col=3)
legend("bottomright", c("Filtered Trajectory", "Smoothed Trajectory"), col=2:3, lty=1)
```
