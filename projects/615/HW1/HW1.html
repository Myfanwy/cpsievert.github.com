<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Homework 1 (due 10 Sep in class)</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<h3>Homework 1 (due 10 Sep in class)</h3>

<p>This homework covers two topics: Bayesian hypothesis testing (question 1) and regularized regression (questions 2-4). For the regularized regression problems, we assume the data are of the form \( y=Xb+e \) where \( e \sim N(0,s^2) \). <strong>Also, define \( ||b||_1 = \sum_{i=1}^p |b_i| \) and \( ||b||_2 = \sum_{i=1}^p b_i^2 \).</strong></p>

<h4>Derive equation (4) in <a href="http://jarad.me/stat615/papers/Ridge_Regression_in_Practice.pdf">Sharpening Ockham&#39;s Razor on a Bayesian Strop</a>.</h4>

<p>\[ 
B = \frac{p(a|E)}{p(a|F)} = \frac{p(a|E)}{\int p(a|\alpha)p(\alpha|F) d\alpha} = \frac{\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(a-42.9)^2}{2\sigma^2}}}{\frac{1}{2\pi\sigma\tau} \int e^{\frac{-(a-\alpha)^2}{2\sigma^2} - \frac{-\alpha^2}{2\tau^2}} d\alpha} = \frac{\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(a-42.9)^2}{2\sigma^2}}}{\frac{1}{\sqrt{2\pi(\sigma^2+\tau^2)}}e^{-\frac{a^2}{2(\sigma^2+\tau^2)}}} = \sqrt{\frac{\sigma^2+\tau^2}{\sigma^2}} e^{\frac{-(a-42.9)^2}{2\sigma^2}+\frac{a^2}{2(\sigma^2+\tau^2)}} = \sqrt{1+\bar{\tau}^2}e^{\frac{-D_E^2}{2}}e^{\frac{D_F^2}{2(1+\bar{\tau}^2)}}
 \]</p>

<p>where \( \bar{\tau} = \frac{\tau}{\sigma}, D_E = \frac{(a - 42.9)}{\sigma}, D_F = \frac{a}{\sigma} \)</p>

<h4>Show that independent normal priors on regression components lead to the ridge estimator on page 5 of <a href="http://jarad.me/stat615/papers/Ridge_Regression_in_Practice.pdf">Ridge Regression in Practice</a>.</h4>

<p>Provided that 
\[ 
\begin{align*}
b &\sim N(\mu = 0, s^2W = \Sigma) = (2\pi)^{\frac{n}{2}}|\Sigma|^{-\frac{1}{2}}exp\{-\frac{1}{2}b'\Sigma^{-1}b\}
\\ s^2 &\sim IG(\alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} s^{-2(\alpha+1)} e^{-\frac{\beta}{s}}
\end{align*}
 \]</p>

<p>\[ 
\begin{align*}
p(b|y) = \int p(b, s^2|y) ds  \propto \int p(y|b, s^2) p(b|s^2) p(s^2) ds &\propto \int f(s) exp\{-\frac{1}{2s^2}(y-Xb)'(y-Xb) -\frac{1}{2}b'\Sigma^{-1}b - \frac{\beta}{s}\} ds
\\ &\propto \int f'(s) exp\{-\frac{1}{2s^2}[||y-Xb||_2 + b'W^{-1}b]\} ds
\\ &\propto \int f'(s) exp\{-\frac{1}{2s^2}[||y-Xb||_2 + ||W^{\frac{-1}{2}}b||_2]\} ds
\end{align*}
 \]</p>

<p>Viewing \( p(b|y) \) as a function of \( b \) and assuming \( W \) is a diagonal matrix, it is clear to see that maximizing \( p(b|y) \) is analagous to usual optimization function for ridge regression: </p>

<p>\[ 
min_{b \in \mathbb{R}} \{||y-Xb||_2 + \lambda^* ||b||_2\}
 \]</p>

<p>where \( \lambda^* = \sqrt{\sum_{i=1}^p \frac{1}{w_{ii}}} \)</p>

<h4>Show that independent <a href="http://en.wikipedia.org/wiki/Laplace_distribution">Laplace</a> priors on regression coefficients lead to the LASSO estimator on page 1 of <a href="http://www.stat.ufl.edu/%7Ecasella/Papers/Lasso.pdf">Bayesian LASSO</a>.</h4>

<p>\[ 
\begin{align*}
b|s^2 &\sim (\frac{\lambda}{2s})^{n} e^{-\frac{\lambda||b||_1}{s}} = \prod_{i=1}^n \frac{\lambda}{2\sqrt{s^2}}e^{\frac{-\lambda|b_i|}{\sqrt{s^2}}}
\\ s^2 &\sim \frac{1}{s^2}
\end{align*}
 \]</p>

<p>\[ 
\begin{align*}
p(b|y) = \int p(b, s^2|y) ds  \propto \int p(y|b, s^2) p(b|s^2) p(s^2) ds &\propto \int g(s) exp\{\frac{-1}{2s^2}||y-Xb||_2\ - \frac{1}{s} \lambda ||b||_1\} ds
\\ &= \int g(s) exp\{\frac{-1}{2s^2}(||y-Xb||_2 + 2s \lambda ||b||_1)\} ds
\end{align*}
 \]</p>

<p>Viewing \( p(b|y) \) as a function of \( b \), it is clear to see that maximizing \( p(b|y) \) is analagous to usual optimization function for the lasso: </p>

<p>\[ 
min_{b \in \mathbb{R}} \{||y-Xb||_2 + \lambda^* ||b||_1\}
 \]</p>

<p>where \( \lambda^* = 2s\lambda \)</p>

<h4>Derive the form of the prior distribution for the <a href="http://en.wikipedia.org/wiki/Elastic_net_regularization">elastic net estimator</a>.</h4>

<p>\[ 
\begin{align*}
b|s^2 &\sim exp\{\frac{-1}{2s^2}(\lambda_1 ||b||_1 + \lambda_2 ||b||_2)\}
\\ s^2 &\sim \frac{1}{s^2}
\end{align*}
 \]</p>

<p>\[ 
p(b|y) = \int p(b, s^2|y) ds  \propto \int p(y|b, s^2) p(b|s^2) p(s^2) ds \propto \int h(s) exp\{\frac{-1}{2s^2}(||y-Xb||_2 + \lambda_1 ||b||_1 + \lambda_2 ||b||_2)\} ds
 \]</p>

<p>Viewing \( p(b|y) \) as a function of \( b \), it is clear to see that maximizing \( p(b|y) \) is analagous to usual optimization function for the elastic net: </p>

<p>\[ 
min_{b \in \mathbb{R}} \{||y-Xb||_2 + \lambda_1 ||b||_1 + \lambda_2 ||b||_2\}
 \]
Adopted from <a href="http://ba.stat.cmu.edu/journal/2010/vol05/issue01/lin.pdf">Li and Lin - 2010</a></p>

</body>

</html>

